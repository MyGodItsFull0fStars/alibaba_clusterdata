{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm_models import LSTM, UtilizationLSTM\n",
    "from gpu_dataloader import ForecastDataset, UtilizationDataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "\n",
    "# plotting the data\n",
    "import matplotlib.pyplot as plt\n",
    "# used for the dataframes\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import get_device\n",
    "\n",
    "import numpy as np\n",
    "from utils import get_device_as_string, get_device, get_rmse, get_mae\n",
    "\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file and load the file\n",
    "with open('./model_configs/tasks_vs_no_tasks/utilization_no_tasks.yaml') as f:\n",
    "    yaml_config = yaml.load(f, Loader=SafeLoader)\n",
    "    print(yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = yaml_config['dataset']['batch_size']\n",
    "small_df: bool = yaml_config['dataset']['small_df']\n",
    "include_tasks: bool = yaml_config['dataset']['include_tasks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UtilizationDataset(is_training=True, small_df=small_df, include_tasks=include_tasks)\n",
    "test_set = UtilizationDataset(is_training=False, small_df=small_df, include_tasks=include_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = yaml_config['model']['num_epochs']\n",
    "learning_rate: float = yaml_config['model']['learning_rate']\n",
    "\n",
    "# number of features\n",
    "input_size: int = dataset.X.shape[2]\n",
    "# number of features in hidden state\n",
    "hidden_size: int = yaml_config['model']['hidden_size']\n",
    "# number of stacked lstm layers\n",
    "num_layers: int = yaml_config['model']['num_layers']\n",
    "# number of output classes\n",
    "num_classes: int = dataset.y.shape[1]\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "INCLUDE_WANDB: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INCLUDE_WANDB == True:\n",
    "    import wandb\n",
    "    wandb.init(project=yaml_config['model']['name'])\n",
    "\n",
    "    wandb.config.num_epochs = num_epochs\n",
    "    wandb.config.learning_rate = learning_rate\n",
    "    wandb.config.input_size = input_size\n",
    "    wandb.config.hidden_size = hidden_size\n",
    "    wandb.config.num_layers = num_layers\n",
    "    wandb.config.num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS: str = 'loss'\n",
    "RMSE_TRAINING: str = 'root mean squared error (training)'\n",
    "MAE_TRAINING: str = 'mean absolute error (training)'\n",
    "\n",
    "if INCLUDE_WANDB:\n",
    "    wandb.define_metric(LOSS, summary='min')\n",
    "    wandb.define_metric(RMSE_TRAINING, summary='min')\n",
    "    wandb.define_metric(MAE_TRAINING, summary='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm = LSTM(num_classes, input_size, hidden_size, num_layers, seq_length, bidirectional=bidirectional)\n",
    "model = UtilizationLSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "model.train()\n",
    "\n",
    "# log gradients and model parameters\n",
    "if INCLUDE_WANDB:\n",
    "    wandb.watch(model)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean square error for regression\n",
    "# nn.\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = RMSELoss()\n",
    "criterion = criterion.to(device)\n",
    "# optimizer function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler_config = yaml_config['model']['scheduler']\n",
    "patience = scheduler_config['patience']\n",
    "factor = scheduler_config['factor']\n",
    "min_lr = scheduler_config['min_lr']\n",
    "eps = scheduler_config['eps']\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=factor, min_lr=min_lr, eps=eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_training_metrics(predictions, labels, loss):\n",
    "    # logging to wandb\n",
    "    if get_device_as_string() == 'cuda' or get_device_as_string() == 'mps':\n",
    "        o = predictions.cpu().detach().numpy()\n",
    "        l = labels.cpu().detach().numpy()\n",
    "    else:\n",
    "        o = predictions.detach().numpy()\n",
    "        l = labels.detach().numpy()\n",
    "    rmse = get_rmse(o, l)\n",
    "    mae = get_mae(o, l)\n",
    "    log_dict: dict = {\n",
    "        LOSS: loss.item(),\n",
    "        RMSE_TRAINING: rmse,\n",
    "        MAE_TRAINING: mae,\n",
    "    }\n",
    "    wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_dataset(dataset: ForecastDataset, batch_size: int):\n",
    "    batch_order = np.array([batch for batch in range(0, len(dataset), batch_size)], dtype=np.int32)\n",
    "    batch_order = np.random.permutation(batch_order)\n",
    "\n",
    "    dataset_order = np.empty(shape=[0, len(dataset.X)], dtype=np.int32)\n",
    "\n",
    "    for batch in batch_order:\n",
    "        if batch >= len(dataset) - (batch_size - 1):\n",
    "            continue\n",
    "        filled_batch_order = np.arange(batch, batch + batch_size, dtype=np.int32)\n",
    "        dataset_order = np.append(dataset_order, filled_batch_order)\n",
    "        \n",
    "    dataset.X = dataset.X[dataset_order]\n",
    "    dataset.y = dataset.y[dataset_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modulo_switch = num_epochs // 10\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "loss_val = None\n",
    "loss_progression: list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.has_cuda:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader: DataLoader) -> float:\n",
    "    predictions, labels, loss = 0, 0, 0\n",
    "    for _, (inputs, labels) in enumerate(train_loader):\n",
    "        # send input and label to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # forward input to model\n",
    "        predictions = model(inputs).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(predictions, labels)\n",
    "        # backward propagation\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    if INCLUDE_WANDB:\n",
    "        log_training_metrics(predictions, labels, loss)\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop():\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(test_set.X.to(device))\n",
    "        val_loss = criterion(val_pred, test_set.y.to(device))\n",
    "        scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in (pbar := tqdm(range(0, num_epochs), desc=f'Training Loop (0) -- Loss: {loss_val}')):\n",
    "\n",
    "    # if epoch % modulo_switch == modulo_switch - 1:\n",
    "    #     reorder_dataset(dataset, batch_size // 2)\n",
    "    #     train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "    \n",
    "    loss_val = training_loop(train_loader)\n",
    "    loss_progression.append(loss_val)\n",
    "    pbar.set_description(f'Training Loop ({epoch + 1}) -- Loss: {loss_val:.5f}')\n",
    "    \n",
    "    validation_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_df = pd.DataFrame(data=loss_progression)\n",
    "# loss_df.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "current_time = time.ctime()\n",
    "current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "if yaml_config['model']['save_model']:\n",
    "    model_name = f'models/epochs-{num_epochs}-{current_time}'\n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': num_epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'input_size': input_size,\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'num_classes': num_classes,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        },\n",
    "        f'{model_name}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_combined_data_df(data_set: UtilizationDataset, save_to_file: bool = True, is_training: bool = True) -> pd.DataFrame:\n",
    "    model.eval()\n",
    "        \n",
    "    X_df = data_set.X.to(device)\n",
    "    label_columns: List[str] = dataset.get_label_columns()\n",
    "    \n",
    "    def get_prediction_df() -> pd.DataFrame:\n",
    "        prediction = model(X_df)\n",
    "        if torch.has_cuda:\n",
    "            prediction = prediction.cpu().detach().numpy()\n",
    "        else:\n",
    "            prediction = prediction.data.numpy()\n",
    "            \n",
    "        prediction_df = pd.DataFrame(prediction, columns=label_columns)\n",
    "        prediction_df = data_set.y_scaler.inverse_normalization_df(prediction_df)\n",
    "        \n",
    "        return prediction_df\n",
    "    \n",
    "    def get_actual_data_df() -> pd.DataFrame:\n",
    "        actual_data = data_set.y.data.numpy()\n",
    "        actual_data_df = pd.DataFrame(actual_data, columns=label_columns)\n",
    "        actual_data_df = data_set.y_scaler.inverse_normalization_df(actual_data_df)\n",
    "        \n",
    "        return actual_data_df\n",
    "    \n",
    "    def get_plan_data_df() -> pd.DataFrame:\n",
    "        plan = data_set.X.cpu()\n",
    "        plan_df = dataset.X_scaler.convert_tensor_to_df(plan)\n",
    "        plan_df = dataset.X_scaler.inverse_standardize_df(plan_df)\n",
    "        plan_df = plan_df[['plan_cpu', 'plan_mem']]\n",
    "        return plan_df\n",
    "    \n",
    "    prediction_df = get_prediction_df()\n",
    "    actual_data_df = get_actual_data_df()\n",
    "    plan_df = get_plan_data_df()\n",
    "    \n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = dataset.X\n",
    "X_df = X_df.to(device)\n",
    "# forward pass\n",
    "prediction = model(X_df)\n",
    "if torch.has_cuda:\n",
    "    prediction = prediction.cpu().detach().numpy()\n",
    "else:\n",
    "    prediction = prediction.data.numpy()\n",
    "\n",
    "actual_data = dataset.y.data.numpy()\n",
    "label_columns = dataset.get_label_columns()\n",
    "\n",
    "# create dataframes\n",
    "prediction_df = pd.DataFrame(prediction, columns=label_columns)\n",
    "actual_data_df = pd.DataFrame(actual_data, columns=label_columns)\n",
    "\n",
    "# reverse transformation\n",
    "prediction_df = dataset.y_scaler.inverse_normalization_df(prediction_df)\n",
    "actual_data_df = dataset.y_scaler.inverse_normalization_df(actual_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = X_df.cpu()\n",
    "plan_df = dataset.X_scaler.convert_tensor_to_df(plan)\n",
    "plan_df = dataset.X_scaler.inverse_standardize_df(plan_df)\n",
    "plan_df = plan_df[['plan_cpu', 'plan_mem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rename_columns_dict: dict = {\n",
    "    'cpu_usage_x': 'actual cpu usage',\n",
    "    'cpu_usage_y': 'predicted cpu usage',\n",
    "    'plan_cpu': 'allocated cpu',\n",
    "    'avg_mem_x': 'actual mem usage',\n",
    "    'avg_mem_y': 'predicted mem usage',\n",
    "    'plan_mem': 'allocated mem'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Root Mean Squared Error\n",
    "\n",
    "Calculating the RMSE for the overall prediction of the (training) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_key: str = 'Root Mean Squared Error (Overall - Training)'\n",
    "rmse_result = get_rmse(actual_data_df[:], prediction_df[:])\n",
    "print(f'Test Score: {rmse_result:.2f} RMSE')\n",
    "if INCLUDE_WANDB:\n",
    "    wandb.summary[rmse_key] = rmse_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Mean Absolute Error\n",
    "\n",
    "Calcutlate the MAE for the overall prediction of the (training) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_key: str = 'Mean Absolute Error (Overall - Training)'\n",
    "mae_result = mean_absolute_error(actual_data_df[:], prediction_df[:])\n",
    "print(f'Test Score: {mae_result} MAE')\n",
    "\n",
    "if INCLUDE_WANDB:\n",
    "    wandb.summary[mae_key] = mae_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(actual_data_df, prediction_df, left_index=True, right_index=True)\n",
    "# combined_df.rename()\n",
    "combined_df[['plan_cpu', 'plan_mem']] = plan_df\n",
    "\n",
    "combined_df = combined_df.rename(columns=rename_columns_dict)\n",
    "\n",
    "combined_df['rmse'] = rmse_result\n",
    "combined_df['mae'] = mae_result\n",
    "\n",
    "if yaml_config['evaluation_path']['save_to_file']:\n",
    "    combined_df.to_csv(yaml_config['evaluation_path']['training_prediction_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_column(actual_values=actual_data_df, predicted_values=prediction_df, column_number: int = 0, rmse_threshold: float = 0.30, is_training: bool = True):\n",
    "\n",
    "    if len(label_columns) <= column_number:\n",
    "        print('Out of Prediction Bounds')\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(25, 15))  # plotting\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "    column = label_columns[column_number]\n",
    "    pred_column = f\"pred_{column}_{'training' if is_training else 'test'}\"\n",
    "\n",
    "    rmse = get_rmse(actual_values[column], predicted_values[column])\n",
    "    mae = mean_absolute_error(actual_values[column], predicted_values[column])\n",
    "\n",
    "    predicted_color = 'green' if rmse < rmse_threshold else 'orange'\n",
    "\n",
    "    plt.plot(actual_values[column], label=column, color='black')  # actual plot\n",
    "    plt.plot(predicted_values[column], label='pred_' +\n",
    "             column, color=predicted_color)  # predicted plot\n",
    "\n",
    "    plt.title('Time-Series Prediction')\n",
    "    plt.plot([], [], ' ', label=f'RMSE: {rmse}')\n",
    "    plt.plot([], [], ' ', label=f'MAE: {mae}')\n",
    "    plt.legend()\n",
    "    plt.ylabel('timeline', fontsize=25)\n",
    "    \n",
    "    if INCLUDE_WANDB:\n",
    "        wandb.log({pred_column: wandb.Image(plt)})\n",
    "        wandb.summary[f'Root Mean Squared Error ({column})'] = rmse\n",
    "        wandb.summary[f'Mean Absolute Error ({column})'] = mae\n",
    "        \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Predictions on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(0, len(label_columns)):\n",
    "#     plot_column(actual_values=actual_data_df, predicted_values=prediction_df, column_number=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Analysis\n",
    "\n",
    "Below, the test set will be loaded and the model evaluated with it to see the actual performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "X_df = test_set.X\n",
    "X_df = X_df.to(device)\n",
    "# forward pass\n",
    "prediction = model(X_df)\n",
    "if get_device_as_string() == 'cuda' or get_device_as_string() == 'mps':\n",
    "    prediction = prediction.cpu().data.numpy()\n",
    "else:\n",
    "    prediction = prediction.data.numpy()\n",
    "\n",
    "actual_data = test_set.y.data.numpy()\n",
    "\n",
    "label_columns = test_set._get_label_columns()\n",
    "\n",
    "# create dataframes\n",
    "prediction_df = pd.DataFrame(prediction, columns=label_columns)\n",
    "actual_data_df = pd.DataFrame(actual_data, columns=label_columns)\n",
    "\n",
    "# reverse transformation\n",
    "prediction_df = dataset.y_scaler.inverse_normalization_df(prediction_df)\n",
    "actual_data_df = dataset.y_scaler.inverse_normalization_df(actual_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = X_df.cpu()\n",
    "plan_test_df = test_set.X_scaler.convert_tensor_to_df(plan)\n",
    "plan_test_df = test_set.X_scaler.inverse_standardize_df(plan_test_df)\n",
    "plan_test_df = plan_df[['plan_cpu', 'plan_mem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_key: str = 'Root Mean Squared Error (Overall - Test)'\n",
    "rmse_result = get_rmse(actual_data_df[:], prediction_df[:])\n",
    "print(f'Test Score: {rmse_result:.2f} RMSE')\n",
    "\n",
    "if INCLUDE_WANDB:\n",
    "    wandb.summary[rmse_key] = rmse_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_key: str = 'Mean Absolute Error (Overall - Test)'\n",
    "mae_result = mean_absolute_error(actual_data_df[:], prediction_df[:])\n",
    "print(f'Test Score: {mae_result} MAE')\n",
    "if INCLUDE_WANDB:\n",
    "    wandb.summary[mae_key] = mae_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_test_df = pd.merge(actual_data_df, prediction_df, left_index=True, right_index=True)\n",
    "# combined_df.rename()\n",
    "combined_test_df[['plan_cpu', 'plan_mem']] = plan_test_df \n",
    "\n",
    "combined_test_df = combined_test_df.rename(columns=rename_columns_dict)\n",
    "\n",
    "combined_test_df['rmse'] = rmse_result\n",
    "combined_test_df['mae'] = mae_result\n",
    "\n",
    "if yaml_config['evaluation_path']['save_to_file']:\n",
    "    combined_test_df.to_csv(yaml_config['evaluation_path']['test_prediction_path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 06:34:44) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc289c9585466324d6bcd715c701435d361dd4760f0e3d7325b29a75549769c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
