{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrispy/miniconda3/envs/ml_pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lstm_models import LSTM\n",
    "from gpu_dataloader import GPUDataset\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# plotting the data\n",
    "import matplotlib.pyplot as plt\n",
    "# used for the dataframes\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "from utils import get_device\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from utils import get_device_as_string, get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_rmse(actual_values, predicted_values) -> float:\n",
    "    '''returns the root mean squared error'''\n",
    "    return math.sqrt(mean_squared_error(actual_values, predicted_values))\n",
    "\n",
    "def get_mape(actual_values, predicted_values):\n",
    "    '''returns the mean absolue percentage error'''\n",
    "    return np.mean(np.abs(actual_values - predicted_values) / np.abs(actual_values) * 100)\n",
    "\n",
    "def get_mae(actual_values, predicted_values) -> float:\n",
    "    '''returns the mean absolute error'''\n",
    "    return mean_absolute_error(actual_values, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = 200\n",
    "dataset = GPUDataset(small_df=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = 3\n",
    "learning_rate: float = 0.001\n",
    "\n",
    "# number of features\n",
    "input_size: int = dataset.X.shape[2]\n",
    "# number of features in hidden state\n",
    "hidden_size: int = dataset.X.shape[2] * 128\n",
    "# number of stacked lstm layers\n",
    "num_layers: int = 1\n",
    "# number of output classes\n",
    "num_classes: int = dataset.y.shape[1]\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmy-god-its-full-of-stars\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/chrispy/Development/alibaba_clusterdata/cluster-trace-gpu-v2020/prediction/wandb/run-20220911_221034-2r0rt56k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/my-god-its-full-of-stars/Hardware%20Utilization%20Prediction/runs/2r0rt56k\" target=\"_blank\">iconic-paper-57</a></strong> to <a href=\"https://wandb.ai/my-god-its-full-of-stars/Hardware%20Utilization%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='Hardware Utilization Prediction')\n",
    "\n",
    "wandb.config.num_epochs = num_epochs\n",
    "wandb.config.learning_rate = learning_rate\n",
    "wandb.config.input_size = input_size\n",
    "wandb.config.hidden_size = hidden_size\n",
    "wandb.config.num_layers = num_layers\n",
    "wandb.config.num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers, dataset.X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x7f16e0ef7be0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOSS: str = 'loss'\n",
    "RMSE_TRAINING: str = 'root mean squared error (training)'\n",
    "MAE_TRAINING: str = 'mean absolute error (training)'\n",
    "\n",
    "wandb.define_metric(LOSS, summary='min')\n",
    "wandb.define_metric(RMSE_TRAINING, summary='min')\n",
    "wandb.define_metric(MAE_TRAINING, summary='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(19, 1216, batch_first=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (fc_1): Linear(in_features=1216, out_features=512, bias=True)\n",
       "  (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc_3): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm = lstm.to(device)\n",
    "lstm.train()\n",
    "# log gradients and model parameters\n",
    "wandb.watch(lstm)\n",
    "\n",
    "lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y) + self.eps)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error for regression\n",
    "# criterion = nn.MSELoss(size_average=5)\n",
    "# criterion = criterion.to(get_device())\n",
    "criterion = RMSELoss()\n",
    "criterion = criterion.to(device)\n",
    "# optimizer function\n",
    "optimizer = torch.optim.AdamW(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_training_metrics(predictions, labels, loss):\n",
    "    # logging to wandb\n",
    "    if get_device_as_string() == 'cuda':\n",
    "        o = predictions.cpu().detach().numpy()\n",
    "        l = labels.cpu().detach().numpy()\n",
    "    else:\n",
    "        o = predictions.detach().numpy()\n",
    "        l = labels.detach().numpy()\n",
    "    rmse = get_rmse(o, l)\n",
    "    mae = get_mae(o, l)\n",
    "    log_dict: dict = {\n",
    "        LOSS: loss.item(),\n",
    "        RMSE_TRAINING: rmse,\n",
    "        MAE_TRAINING: mae,\n",
    "    }\n",
    "    wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   1%|          | 1/100 [00:11<18:11, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.017037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   2%|▏         | 2/100 [00:22<17:58, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.020168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   3%|▎         | 3/100 [00:33<17:50, 11.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, loss: 0.070664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   4%|▍         | 4/100 [00:44<17:47, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, loss: 0.031968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loop:   5%|▌         | 5/100 [00:55<17:36, 11.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, loss: 0.018207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f1803697040>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chrispy/miniconda3/envs/ml_pytorch/lib/python3.9/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "Training Loop:   6%|▌         | 6/100 [01:06<17:30, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, loss: 0.004633\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in tqdm(range(0, num_epochs), desc=\"Training Loop\"):\n",
    "\n",
    "    batch_section = [idx for idx in range(len(dataset) // batch_size)]\n",
    "\n",
    "    while len(batch_section) > 0:\n",
    "        choice = random.choice(batch_section)\n",
    "        sampler = SubsetRandomSampler(\n",
    "            list(range(choice*batch_size, (choice+1)*batch_size)))\n",
    "        train_loader = DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=False, num_workers=6, sampler=sampler)\n",
    "        \n",
    "        # train_loader = tqdm(train_loader)\n",
    "\n",
    "        for _, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = lstm.forward(inputs)\n",
    "            predictions = predictions.to(device)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            log_training_metrics(predictions, labels, loss)\n",
    "\n",
    "        # remove batch section after training on it\n",
    "        batch_section.remove(choice)\n",
    "        \n",
    "\n",
    "    print(f'Epoch: {epoch}, loss: {loss.item():2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "current_time = time.ctime()\n",
    "current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(lstm.state_dict(), f'models/epochs-{num_epochs}-{current_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.eval()\n",
    "\n",
    "X_df = dataset.X\n",
    "X_df = X_df.to(device)\n",
    "# forward pass\n",
    "prediction = lstm(X_df)\n",
    "if get_device_as_string() == 'cuda':\n",
    "    prediction = prediction.cpu().data.numpy()\n",
    "else:\n",
    "    prediction = prediction.data.numpy()\n",
    "prediction = dataset.standard_scaler.fit_transform(prediction)\n",
    "\n",
    "actual_data = dataset.y.data.numpy()\n",
    "actual_data = dataset.minmax_scaler.fit_transform(actual_data)\n",
    "\n",
    "# reverse transformation\n",
    "prediction = dataset.standard_scaler.inverse_transform(prediction)\n",
    "actual_data = dataset.minmax_scaler.inverse_transform(actual_data)\n",
    "\n",
    "label_columns = dataset.get_default_label_columns()\n",
    "# create dataframes\n",
    "prediction_df = pd.DataFrame(prediction, columns=label_columns)\n",
    "actual_data_df = pd.DataFrame(actual_data, columns=label_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Root Mean Squared Error\n",
    "\n",
    "Calculating the RMSE for the overall prediction of the (training) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_key: str = 'Root Mean Squared Error (Overall - Training)'\n",
    "rmse_result = get_rmse(actual_data_df[:], prediction_df[:])\n",
    "print(f'Test Score: {rmse_result:.2f} RMSE')\n",
    "wandb.summary[rmse_key] = rmse_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Mean Absolute Error\n",
    "\n",
    "Calcutlate the MAE for the overall prediction of the (training) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_key: str = 'Mean Absolute Error (Overall - Training)'\n",
    "mae_result = mean_absolute_error(actual_data_df[:], prediction_df[:])\n",
    "print(f'Test Score: {mae_result} MAE')\n",
    "wandb.summary[mae_key] = mae_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_column(actual_values=actual_data_df, predicted_values=prediction_df, column_number: int = 0, rmse_threshold: float = 0.30, is_training: bool = True):\n",
    "\n",
    "    if len(label_columns) <= column_number:\n",
    "        print('Out of Prediction Bounds')\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(25, 15))  # plotting\n",
    "\n",
    "    column = label_columns[column_number]\n",
    "    pred_column = f\"pred_{column}_{'training' if is_training else 'test'}\"\n",
    "\n",
    "    rmse = get_rmse(actual_values[column], predicted_values[column])\n",
    "    mae = mean_absolute_error(actual_values[column], predicted_values[column])\n",
    "\n",
    "    predicted_color = 'green' if rmse < rmse_threshold else 'orange'\n",
    "\n",
    "    plt.plot(actual_values[column], label=column, color='black')  # actual plot\n",
    "    plt.plot(predicted_values[column], label='pred_' +\n",
    "             column, color=predicted_color)  # predicted plot\n",
    "\n",
    "    plt.title('Time-Series Prediction')\n",
    "    plt.plot([], [], ' ', label=f'RMSE: {rmse}')\n",
    "    plt.plot([], [], ' ', label=f'MAE: {mae}')\n",
    "    plt.legend()\n",
    "    \n",
    "    wandb.log({pred_column: wandb.Image(plt)})\n",
    "    wandb.summary[f'Root Mean Squared Error ({column})'] = rmse\n",
    "    wandb.summary[f'Mean Absolute Error ({column})'] = mae\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Predictions on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Analysis\n",
    "\n",
    "Below, the test set will be loaded and the model evaluated with it to see the actual performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = GPUDataset(is_training=False, small_df=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm.eval()\n",
    "\n",
    "X_df = test_dataset.X\n",
    "X_df = X_df.to(device)\n",
    "# forward pass\n",
    "prediction = lstm(X_df)\n",
    "if get_device_as_string() == 'cuda':\n",
    "    prediction = prediction.cpu().data.numpy()\n",
    "else:\n",
    "    prediction = prediction.data.numpy()\n",
    "prediction = test_dataset.standard_scaler.fit_transform(prediction)\n",
    "\n",
    "actual_data = test_dataset.y.data.numpy()\n",
    "actual_data = test_dataset.minmax_scaler.fit_transform(actual_data)\n",
    "\n",
    "# reverse transformation\n",
    "prediction = test_dataset.standard_scaler.inverse_transform(prediction)\n",
    "actual_data = test_dataset.minmax_scaler.inverse_transform(actual_data)\n",
    "\n",
    "label_columns = test_dataset.get_default_label_columns()\n",
    "# create dataframes\n",
    "prediction_df = pd.DataFrame(prediction, columns=label_columns)\n",
    "actual_data_df = pd.DataFrame(actual_data, columns=label_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_key: str = 'Root Mean Squared Error (Overall - Test)'\n",
    "rmse_result = get_rmse(actual_data_df[:], prediction_df[:])\n",
    "print(f'Test Score: {rmse_result:.2f} RMSE')\n",
    "wandb.summary[rmse_key] = rmse_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_key: str = 'Mean Absolute Error (Overall - Test)'\n",
    "mae_result = mean_absolute_error(actual_data_df[:], prediction_df[:])\n",
    "print(f'Test Score: {mae_result} MAE')\n",
    "wandb.summary[mae_key] = mae_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(column_number=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "631fa29ff452e796f5703a13b9a25aaedc38e1bbca43d17166395e7a7dc8b7e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
