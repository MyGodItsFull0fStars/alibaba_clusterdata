{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmy-god-its-full-of-stars\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/macbook/Development/alibaba_clusterdata/cluster-trace-gpu-v2020/prediction/wandb/run-20220907_140826-lgt58wd7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/my-god-its-full-of-stars/Hardware%20Utilization%20Prediction/runs/lgt58wd7\" target=\"_blank\">deep-dew-25</a></strong> to <a href=\"https://wandb.ai/my-god-its-full-of-stars/Hardware%20Utilization%20Prediction\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# used for statistical processes, i.e scaling the dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# plotting the data\n",
    "import matplotlib.pyplot as plt\n",
    "# used for the dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# transforming dataframes into arrays\n",
    "# and those arrays to Tensors, the ML approach can work with\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# required for the LSTM model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.init(project='Hardware Utilization Prediction')\n",
    "\n",
    "from gpu_dataloader import GPUDataset\n",
    "\n",
    "from lstm_models import LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/macbook/Development/alibaba_clusterdata/cluster-trace-gpu-v2020/prediction/lstm_general_prediction.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/macbook/Development/alibaba_clusterdata/cluster-trace-gpu-v2020/prediction/lstm_general_prediction.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m GPUDataset()\n",
      "File \u001b[0;32m~/Development/alibaba_clusterdata/cluster-trace-gpu-v2020/prediction/gpu_dataloader.py:30\u001b[0m, in \u001b[0;36mGPUDataset.__init__\u001b[0;34m(self, data_path, data_index, feature_columns, label_columns, batch_size, small_df)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmall_df \u001b[39m=\u001b[39m small_df\n\u001b[0;32m---> 30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_dataset(\n\u001b[1;32m     31\u001b[0m     data_path, data_index)\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Development/alibaba_clusterdata/cluster-trace-gpu-v2020/prediction/gpu_dataloader.py:54\u001b[0m, in \u001b[0;36mGPUDataset.__prepare_dataset\u001b[0;34m(self, data_path, data_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__prepare_dataset\u001b[39m(\u001b[39mself\u001b[39m, data_path: \u001b[39mstr\u001b[39m, data_index: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_dataframe(data_path, data_index)\n",
      "File \u001b[0;32m~/Development/alibaba_clusterdata/cluster-trace-gpu-v2020/prediction/gpu_dataloader.py:74\u001b[0m, in \u001b[0;36mGPUDataset.__prepare_dataframe\u001b[0;34m(self, data_path, data_index, drop_columns)\u001b[0m\n\u001b[1;32m     61\u001b[0m df\u001b[39m.\u001b[39mset_index(data_index)\n\u001b[1;32m     63\u001b[0m \u001b[39m# One-Hot Encoding\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39m# dummies = pd.get_dummies(df.task_name)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# df = df.join(dummies)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[39m# df.sort_index(inplace=True)\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__append_to_feature_and_label_set(df)\n",
      "File \u001b[0;32m~/Development/alibaba_clusterdata/cluster-trace-gpu-v2020/prediction/gpu_dataloader.py:128\u001b[0m, in \u001b[0;36mGPUDataset.__append_to_feature_and_label_set\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     X_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(\n\u001b[1;32m    127\u001b[0m         [X_df, df\u001b[39m.\u001b[39miloc[feature_start_index:feature_end_index]])\n\u001b[0;32m--> 128\u001b[0m     y_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(\n\u001b[1;32m    129\u001b[0m         [y_df, df\u001b[39m.\u001b[39miloc[label_start_index:label_end_index]])\n\u001b[1;32m    131\u001b[0m X_df, y_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__filter_columns(X_df, y_df)\n\u001b[1;32m    132\u001b[0m X_df, y_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__scale_dfs(X_df, y_df)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = GPUDataset(small_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = 100\n",
    "learning_rate: float = 0.015\n",
    "\n",
    "# number of features\n",
    "input_size: int = dataset.X.shape[2]\n",
    "# number of features in hidden state\n",
    "hidden_size: int = dataset.X.shape[2] * 8\n",
    "# number of stacked lstm layers\n",
    "num_layers: int = 1\n",
    "# number of output classes\n",
    "\n",
    "num_classes: int = dataset.y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config.num_epochs = num_epochs\n",
    "wandb.config.learning_rate = learning_rate\n",
    "wandb.config.input_size = input_size\n",
    "wandb.config.hidden_size = hidden_size\n",
    "wandb.config.num_layers = num_layers\n",
    "wandb.config.num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers, dataset.X.shape[1])\n",
    "lstm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error for regression\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer function\n",
    "optimizer = torch.optim.AdamW(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_rmse(actual_values, predicted_values) -> float:\n",
    "    '''returns the root mean squared error'''\n",
    "    return math.sqrt(mean_squared_error(actual_values, predicted_values))\n",
    "\n",
    "def get_mape(actual_values, predicted_values):\n",
    "    '''returns the mean absolue percentage error'''\n",
    "    return np.mean(np.abs(actual_values - predicted_values) / np.abs(actual_values) * 100)\n",
    "\n",
    "def get_mae(actual_values, predicted_values) -> float:\n",
    "    '''returns the mean absolute error'''\n",
    "    return mean_absolute_error(actual_values, predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS: str = 'loss'\n",
    "RMSE_TRAINING: str = 'root mean squared error (training)'\n",
    "MAE_TRAINING: str = 'mean absolute error (training)'\n",
    "\n",
    "wandb.define_metric(LOSS, summary='min')\n",
    "wandb.define_metric(RMSE_TRAINING, summary='min')\n",
    "wandb.define_metric(MAE_TRAINING, summary='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "batch_size: int = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch: {epoch + 1}')\n",
    "\n",
    "    sample_idx = [idx for idx in range(len(dataset) // batch_size)]\n",
    "\n",
    "    while len(sample_idx) > 0:\n",
    "        choice = random.choice(sample_idx)\n",
    "        sampler = SubsetRandomSampler(\n",
    "            list(range(choice*batch_size, (choice+1)*batch_size)))\n",
    "        train_loader = DataLoader(\n",
    "            dataset, batch_size=batch_size, shuffle=False, num_workers=5, sampler=sampler)\n",
    "\n",
    "        for _, (inputs, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = lstm.forward(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        sample_idx.remove(choice)\n",
    "        print(f'removed {choice}, remaining: {len(sample_idx)}')\n",
    "\n",
    "    # logging to wandb\n",
    "    o = outputs.detach()\n",
    "    rmse = get_rmse(o, labels)\n",
    "    mae = get_mae(o, labels)\n",
    "    log_dict: dict = {\n",
    "        LOSS: loss.item(),\n",
    "        RMSE_TRAINING: rmse,\n",
    "        MAE_TRAINING: mae,\n",
    "    }\n",
    "    wandb.log(log_dict)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f'Epoch: {epoch + 1}, loss: {loss.item():2f}, rmse: {rmse:2f}, mae: {mae:2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc289c9585466324d6bcd715c701435d361dd4760f0e3d7325b29a75549769c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
