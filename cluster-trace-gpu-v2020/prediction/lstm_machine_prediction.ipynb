{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from lstm_models import LSTM, UtilizationLSTM\n",
    "from gpu_dataloader import MachineDatasetContainer, MachineDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# used for the dataframes\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml\n",
    "from yaml.loader import SafeLoader\n",
    "\n",
    "from utils import get_device, get_rmse, get_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model_configs/tasks_vs_no_tasks/utilization_no_tasks.yaml') as file:\n",
    "    yaml_config = yaml.load(file, Loader=SafeLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size: int = yaml_config['dataset']['batch_size']\n",
    "small_df: bool = yaml_config['dataset']['small_df']\n",
    "include_tasks: bool = yaml_config['dataset']['include_tasks']\n",
    "batch_size, small_df, include_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MachineDatasetContainer(is_training=True, small_df=small_df, include_tasks=include_tasks)\n",
    "test_dataset = MachineDatasetContainer(is_training=False, small_df=small_df, include_tasks=include_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = yaml_config['model']['num_epochs']\n",
    "learning_rate: float = yaml_config['model']['learning_rate']\n",
    "\n",
    "input_size: int = dataset.get_model_input_size()\n",
    "hidden_size: int = yaml_config['model']['hidden_size']\n",
    "num_layers: int = yaml_config['model']['num_layers']\n",
    "num_classes: int = dataset.get_model_num_classes()\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "INCLUDE_WANDB: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INCLUDE_WANDB == True:\n",
    "    import wandb\n",
    "    wandb.init(project=yaml_config['model']['name'])\n",
    "\n",
    "    wandb.config.num_epochs = num_epochs\n",
    "    wandb.config.learning_rate = learning_rate\n",
    "    wandb.config.input_size = input_size\n",
    "    wandb.config.hidden_size = hidden_size\n",
    "    wandb.config.num_layers = num_layers\n",
    "    wandb.config.num_classes = num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS: str = 'loss'\n",
    "RMSE_TRAINING: str = 'root mean squared error (training)'\n",
    "MAE_TRAINING: str = 'mean absolute error (training)'\n",
    "\n",
    "if INCLUDE_WANDB:\n",
    "    wandb.define_metric(LOSS, summary='min')\n",
    "    wandb.define_metric(RMSE_TRAINING, summary='min')\n",
    "    wandb.define_metric(MAE_TRAINING, summary='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UtilizationLSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "model.train()\n",
    "\n",
    "if INCLUDE_WANDB:\n",
    "    wandb.watch(model)\n",
    "    \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler_config = yaml_config['model']['scheduler']\n",
    "patience = scheduler_config['patience']\n",
    "factor = scheduler_config['factor']\n",
    "min_lr = scheduler_config['min_lr']\n",
    "eps = scheduler_config['eps']\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=factor, min_lr=min_lr, eps=eps)\n",
    "del patience, factor, min_lr, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_training_metrics(predictions, labels, loss):\n",
    "    # logging to wandb\n",
    "    if torch.cuda.is_available():\n",
    "        o = predictions.cpu().detach().numpy()\n",
    "        l = labels.cpu().detach().numpy()\n",
    "    else:\n",
    "        o = predictions.detach().numpy()\n",
    "        l = labels.detach().numpy()\n",
    "    rmse = get_rmse(o, l)\n",
    "    mae = get_mae(o, l)\n",
    "    log_dict: dict = {\n",
    "        LOSS: loss.item(),\n",
    "        RMSE_TRAINING: rmse,\n",
    "        MAE_TRAINING: mae,\n",
    "    }\n",
    "    wandb.log(log_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_datasets_to_data_loaders(data: MachineDatasetContainer, shuffle: bool = False, num_workers: int = 5) -> List[DataLoader]:\n",
    "    data_loaders: List[DataLoader] = list()\n",
    "    for dataset in data.dataset_list:\n",
    "        data_loaders.append(\n",
    "            DataLoader(dataset, batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "        )\n",
    "        \n",
    "    return data_loaders\n",
    "\n",
    "train_data_loaders: List[DataLoader] = convert_datasets_to_data_loaders(dataset)\n",
    "test_data_loaders: List[DataLoader] = convert_datasets_to_data_loaders(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.has_cuda:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader_training_loop(train_loader: DataLoader) -> float:\n",
    "    predictions, labels, loss = 0, 0, 0\n",
    "    \n",
    "    for _, (inputs, labels) in enumerate(train_loader):\n",
    "        # send input and label to device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # forward input to model\n",
    "        predictions = model(inputs).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(predictions, labels)\n",
    "        # backward propagation\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    if INCLUDE_WANDB:\n",
    "        log_training_metrics(predictions, labels, loss)\n",
    "        \n",
    "    return loss.item()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_loop():\n",
    "    with torch.no_grad():\n",
    "        for test_loader in test_data_loaders:\n",
    "            for _, (inputs, labels) in enumerate(test_loader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                val_prediction = model(inputs).to(device)\n",
    "                val_loss = criterion(val_prediction, labels)\n",
    "                scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val = None\n",
    "loss_progression: list = list()\n",
    "\n",
    "for epoch in (pbar := tqdm(range(0, 15), desc=f'Training Loop (0) -- Loss: {loss_val}')):\n",
    "    \n",
    "    for train_loader in train_data_loaders:\n",
    "        \n",
    "        loss_val = train_loader_training_loop(train_loader)\n",
    "        loss_progression.append(loss_val)\n",
    "        \n",
    "        pbar.set_description(f'Training Loop ({epoch + 1}) -- Loss: {loss_val:.5f}')\n",
    "        \n",
    "    validation_loop()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = time.ctime()\n",
    "current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "if yaml_config['model']['save_model'] and False:\n",
    "    model_name = f'models/epochs-{num_epochs}-{current_time}'\n",
    "    torch.save(\n",
    "        {\n",
    "            'epoch': num_epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'input_size': input_size,\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'num_classes': num_classes,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            \n",
    "        },\n",
    "        f'{model_name}'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_data_df(dataset_container: MachineDatasetContainer, save_to_file: bool = True) -> pd.DataFrame:\n",
    "    model.eval()\n",
    "    prediction_list: List[pd.DataFrame] = list()\n",
    "    actual_data_list: List[pd.DataFrame] = list()\n",
    "    plan_data_list: List[pd.DataFrame] = list()\n",
    "    \n",
    "    def add_prediction(dataset: MachineDataset, dst: List[pd.DataFrame]):\n",
    "        X_df = dataset.X.to(device)\n",
    "        pred = model(X_df)\n",
    "        \n",
    "        if torch.has_cuda:\n",
    "            pred = pred.cpu().detach().numpy()\n",
    "        else: \n",
    "            pred = pred.data.numpy()\n",
    "            \n",
    "        label_columns = dataset.label_columns\n",
    "        \n",
    "        prediction_df = pd.DataFrame(pred, columns=label_columns)\n",
    "        prediction_df = dataset.y_scaler.inverse_normalization_df(prediction_df)\n",
    "        prediction_list.append(prediction_df)\n",
    "        \n",
    "    def add_actual_data(dataset: MachineDataset, dst: List[pd.DataFrame]):\n",
    "        label_columns = dataset.label_columns\n",
    "        actual_data = dataset.y.data.numpy()\n",
    "        actual_data_df = pd.DataFrame(actual_data, columns=label_columns)\n",
    "        actual_data_df = dataset.y_scaler.inverse_normalization_df(actual_data_df)\n",
    "        actual_data_list.append(actual_data_df)\n",
    "    \n",
    "    def add_plan_data(dataset: MachineDataset, dst: List[pd.DataFrame]):\n",
    "        plan_df = dataset.X.cpu()\n",
    "        plan_df = dataset.X_scaler.convert_tensor_to_df(plan_df)\n",
    "        plan_df = dataset.X_scaler.inverse_standardize_df(plan_df)\n",
    "        plan_df = plan_df[['plan_cpu', 'plan_mem']]\n",
    "        plan_data_list.append(plan_df)\n",
    "        \n",
    "    def get_rename_columns() -> dict:\n",
    "        return {\n",
    "            'cpu_usage_x': 'actual cpu usage', \n",
    "            'cpu_usage_y': 'predicted cpu usage', \n",
    "            'plan_cpu': 'allocated cpu',\n",
    "            'avg_mem_x': 'actual mem usage',\n",
    "            'avg_mem_y': 'predicted mem usage',\n",
    "            'plan_mem': 'allocated mem'\n",
    "            }\n",
    "        \n",
    "    def combine_dataframes(pred_df, actual_data_df, plan_df) -> pd.DataFrame:\n",
    "        combined_df = pd.merge(actual_data_df, pred_df, left_index=True, right_index=True)\n",
    "        combined_df[['plan_cpu', 'plan_mem']] = plan_df\n",
    "        combined_df = combined_df.rename(columns=get_rename_columns())\n",
    "        \n",
    "        return combined_df\n",
    "    \n",
    "    for ds in dataset_container.dataset_list:\n",
    "        add_prediction(ds, prediction_list)\n",
    "        add_actual_data(ds, actual_data_list)\n",
    "        add_plan_data(ds, plan_data_list)\n",
    "        \n",
    "    prediction_df = pd.concat(prediction_list, ignore_index=True)\n",
    "    actual_data_df = pd.concat(actual_data_list, ignore_index=True)\n",
    "    plan_df = pd.concat(plan_data_list, ignore_index=True)\n",
    "    \n",
    "    combined_df = combine_dataframes(prediction_df, actual_data_df, plan_df)\n",
    "\n",
    "    if save_to_file and yaml_config['evaluation_path']['save_to_file']:\n",
    "        combined_df.to_csv(yaml_config['evaluation_path']['training_prediction_path'])   \n",
    "        \n",
    "    del prediction_list, actual_data_list, plan_data_list\n",
    "    \n",
    "    return combined_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_df = get_combined_data_df(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined_df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc289c9585466324d6bcd715c701435d361dd4760f0e3d7325b29a75549769c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
